{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjCIuW6mttsJ2MKwvn7i5t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwhite12/Colab-Notebooks/blob/main/Uplight_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "-13UeR1rzzQ1",
        "outputId": "a24c2324-c285-4191-90a5-fc3ba973c6a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,582 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,750 kB]\n",
            "Fetched 5,459 kB in 1s (3,903 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-4.0.1.tar.gz (434.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.2/434.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.9 (from pyspark)\n",
            "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813800 sha256=5abdf5805780093c44917eecd8059b5404feb55c655d7e6a22236f6f310d0ed0\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9f/68/f89fb34ccd886909be7d0e390eaaf97f21efdf540c0ee8dbcd\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 4.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed py4j-0.10.9.9 pyspark-4.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "py4j",
                  "pyspark"
                ]
              },
              "id": "838bad0d83934af195320cb96f4d3a7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "#install requirements\n",
        "!sudo apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.7/spark-3.5.7-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.7-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install --upgrade pyspark\n",
        "!pip install py4j\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create spark session\n",
        "import os\n",
        "import sys\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.7-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.testing.utils import assertDataFrameEqual as ADF\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Energy Usage Pipeline\") \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "5BLtTWtX0T_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the provided CSV from a local file path\n",
        "#Validate the schema (all columns have expected types). Handle or log invalid records.\n",
        "valid_schema = T.StructType([\n",
        "    T.StructField(\"customer_id\", T.IntegerType(), True),\n",
        "    T.StructField(\"usage_kwh\", T.DoubleType(), False),\n",
        "    T.StructField(\"start_time\", T.StringType(), True),\n",
        "    T.StructField(\"_corrupt_record\", T.StringType(), True)\n",
        "])\n",
        "\n",
        "#log invalid records using permissive mode\n",
        "df = spark.read \\\n",
        "    .schema(valid_schema) \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"mode\", \"PERMISSIVE\") \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(valid_schema) \\\n",
        "    .load(\"/energy_usage.csv\")\n",
        "\n",
        "#df = spark.read.load(\"/content/energy_usage.csv\", format=\"csv\", header=\"true\", schema=valid_schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3TfoTCFRwAG",
        "outputId": "e1af6ed9-8923-4652-eafd-8b82604453b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+--------------------+--------------------+\n",
            "|customer_id|usage_kwh|          start_time|     _corrupt_record|\n",
            "+-----------+---------+--------------------+--------------------+\n",
            "|        123|      5.1|2023-05-12T01:00:...|                NULL|\n",
            "|        456|      4.3|2023-05-12T01:30:...|                NULL|\n",
            "|        123|      2.2|2023-05-12T02:00:...|                NULL|\n",
            "|        789|     NULL|2023-05-11T23:00:...|                NULL|\n",
            "|       NULL|      1.1|2023-05-12T01:00:...|abc,1.1,2023-05-1...|\n",
            "|        123|      NaN|2023-05-12T03:00:...|                NULL|\n",
            "|        456|      8.9|2023-05-12T06:00:...|                NULL|\n",
            "+-----------+---------+--------------------+--------------------+\n",
            "\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- usage_kwh: double (nullable = true)\n",
            " |-- start_time: string (nullable = true)\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform the data:\n",
        "  #Convert `start_time` to UTC datetime.\n",
        "df = df.withColumn(\"start_time\", F.col(\"start_time\").cast(\"timestamp\"))\n",
        "time_offset = \"-07:00\"\n",
        "df_utc = df.withColumn(\"start_time_utc\", F.to_utc_timestamp(F.col(\"start_time\"), time_offset))\n",
        "df_utc.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTQI61tkgIR8",
        "outputId": "8f1b5fdc-6fd3-4530-91b0-c5d26dec96d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-------------------+--------------------+-------------------+\n",
            "|customer_id|usage_kwh|         start_time|     _corrupt_record|     start_time_utc|\n",
            "+-----------+---------+-------------------+--------------------+-------------------+\n",
            "|        123|      5.1|2023-05-12 08:00:00|                NULL|2023-05-12 15:00:00|\n",
            "|        456|      4.3|2023-05-12 08:30:00|                NULL|2023-05-12 15:30:00|\n",
            "|        123|      2.2|2023-05-12 09:00:00|                NULL|2023-05-12 16:00:00|\n",
            "|        789|     NULL|2023-05-12 06:00:00|                NULL|2023-05-12 13:00:00|\n",
            "|       NULL|      1.1|2023-05-12 08:00:00|abc,1.1,2023-05-1...|2023-05-12 15:00:00|\n",
            "|        123|      NaN|2023-05-12 10:00:00|                NULL|2023-05-12 17:00:00|\n",
            "|        456|      8.9|2023-05-12 13:00:00|                NULL|2023-05-12 20:00:00|\n",
            "+-----------+---------+-------------------+--------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregate each customer's total `usage_kwh` per day (UTC).\n",
        "#filter out corrupt records\n",
        "df_usage_clean = df_utc.filter(df_utc._corrupt_record.isNull())\n",
        "#fill NULL and NA with 0; you could also drop these depending on the use case\n",
        "df_usage_clean = df_usage_clean.na.fill(value=0)\n",
        "df_usage_clean.show()\n",
        "df_usage_agg = df_usage_clean.groupBy(F.to_date(F.col(\"start_time_utc\"), \"MM/dd/yyyy\").alias(\"date\"), \"customer_id\").sum(\"usage_kwh\")\n",
        "df_usage_agg.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGNGJuKonS3Q",
        "outputId": "6756e617-04b4-44a1-e225-4535bd152011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-------------------+---------------+-------------------+\n",
            "|customer_id|usage_kwh|         start_time|_corrupt_record|     start_time_utc|\n",
            "+-----------+---------+-------------------+---------------+-------------------+\n",
            "|        123|      5.1|2023-05-12 08:00:00|           NULL|2023-05-12 15:00:00|\n",
            "|        456|      4.3|2023-05-12 08:30:00|           NULL|2023-05-12 15:30:00|\n",
            "|        123|      2.2|2023-05-12 09:00:00|           NULL|2023-05-12 16:00:00|\n",
            "|        789|      0.0|2023-05-12 06:00:00|           NULL|2023-05-12 13:00:00|\n",
            "|        123|      0.0|2023-05-12 10:00:00|           NULL|2023-05-12 17:00:00|\n",
            "|        456|      8.9|2023-05-12 13:00:00|           NULL|2023-05-12 20:00:00|\n",
            "+-----------+---------+-------------------+---------------+-------------------+\n",
            "\n",
            "+----------+-----------+--------------+\n",
            "|      date|customer_id|sum(usage_kwh)|\n",
            "+----------+-----------+--------------+\n",
            "|2023-05-12|        789|           0.0|\n",
            "|2023-05-12|        123|           7.3|\n",
            "|2023-05-12|        456|          13.2|\n",
            "+----------+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Output the cleaned & aggregated data as a Parquet or Delta file to a specified output path.\n",
        "file_output = \"/energy_usage.parquet\"\n",
        "df_usage_agg.write.parquet(file_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "9uWfhZNagxLO",
        "outputId": "7e45bb9c-3878-4036-ac7c-e4790c2e6c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_ALREADY_EXISTS] Path file:/energy_usage.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1288111507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Output the cleaned & aggregated data as a Parquet or Delta file to a specified output path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/energy_usage.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_usage_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.5.7-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     def text(\n",
            "\u001b[0;32m/content/spark-3.5.7-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.7-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/energy_usage.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#**Testing:** Add one unit test for your transformation logic.\n",
        "#def test_transformation_logic(spark):\n",
        "    #'''\n",
        "   # Test conversion to UTC and aggregation\n",
        "    #'''\n",
        "test_rows = [(1, 4.2, \"2025-09-01T12:45:00-07:00\"),\n",
        "             (1, 2.3, \"2025-09-01T19:29:00-07:00\"),\n",
        "             (1, 6.4, \"2025-09-02T13:12:00-07:00\"),\n",
        "             (2, 3.7, \"2025-09-02T08:33:00-07:00\"),\n",
        "             (3, 8.3, \"2025-09-02T14:22:00-07:00\"),\n",
        "             (4, 3.1, \"2025-09-02T16:42:00-07:00\"),\n",
        "             (4, 1.2, \"2025-09-02T21:30:00-07:00\"),\n",
        "             (5, 5.0, \"2025-09-02T22:19:00-07:00\")\n",
        "            ]\n",
        "\n",
        "test_columns = T.StructType(\n",
        "    [T.StructField(\"customer_id\", T.IntegerType(), False),\n",
        "     T.StructField(\"usage_kwh\", T.DoubleType(), False),\n",
        "     T.StructField(\"start_time\", T.StringType(), False)\n",
        "     ])\n",
        "\n",
        "test_df = spark.createDataFrame(data=test_rows, schema=test_columns)\n",
        "test_df.show()\n",
        "\n",
        "#expected output\n",
        "expected_rows = [(\"2025-09-02\", 1, 6.5),\n",
        "                 (\"2025-09-02\", 2, 3.7),\n",
        "                 (\"2025-09-03\", 1, 6.4),\n",
        "                 (\"2025-09-03\", 3, 8.3),\n",
        "                 (\"2025-09-03\", 4, 4.3),\n",
        "                 (\"2025-09-03\", 5, 5.0)\n",
        "                 ]\n",
        "\n",
        "expected_columns = T.StructType(\n",
        "    [T.StructField(\"date\", T.StringType(), False),\n",
        "     T.StructField(\"customer_id\", T.IntegerType(), False),\n",
        "     T.StructField(\"sum_usage_kwh\", T.DoubleType(), False)\n",
        "    ])\n",
        "\n",
        "expected_df = spark.createDataFrame(data=expected_rows, schema=expected_columns)\n",
        "expected_df = expected_df.withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n",
        "expected_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou2pktGBgzUo",
        "outputId": "9790e056-719b-4730-fe68-1ec7d9ef2bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+--------------------+\n",
            "|customer_id|usage_kwh|          start_time|\n",
            "+-----------+---------+--------------------+\n",
            "|          1|      4.2|2025-09-01T12:45:...|\n",
            "|          1|      2.3|2025-09-01T19:29:...|\n",
            "|          1|      6.4|2025-09-02T13:12:...|\n",
            "|          2|      3.7|2025-09-02T08:33:...|\n",
            "|          3|      8.3|2025-09-02T14:22:...|\n",
            "|          4|      3.1|2025-09-02T16:42:...|\n",
            "|          4|      1.2|2025-09-02T21:30:...|\n",
            "|          5|      5.0|2025-09-02T22:19:...|\n",
            "+-----------+---------+--------------------+\n",
            "\n",
            "+----------+-----------+-------------+\n",
            "|      date|customer_id|sum_usage_kwh|\n",
            "+----------+-----------+-------------+\n",
            "|2025-09-02|          1|          6.5|\n",
            "|2025-09-02|          2|          3.7|\n",
            "|2025-09-03|          1|          6.4|\n",
            "|2025-09-03|          3|          8.3|\n",
            "|2025-09-03|          4|          4.3|\n",
            "|2025-09-03|          5|          5.0|\n",
            "+----------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform the data:\n",
        "  #Convert `start_time` to UTC datetime.\n",
        "test_df = test_df.withColumn(\"start_time\", F.col(\"start_time\").cast(\"timestamp\"))\n",
        "time_offset = \"-07:00\"\n",
        "test_df_utc = test_df.withColumn(\"start_time_utc\", F.to_utc_timestamp(F.col(\"start_time\"), time_offset))\n",
        "test_df_utc.show()\n",
        "\n",
        "test_df_agg = test_df_utc.groupBy(F.to_date(F.col(\"start_time_utc\"), \\\n",
        "                                            \"MM/dd/yyyy\").alias(\"date\"), \"customer_id\").sum(\"usage_kwh\") \\\n",
        "                                            .orderBy(\"date\", \"customer_id\")\n",
        "test_df_agg.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dyPFL4PrX4Q",
        "outputId": "68c30750-890e-4ca6-9181-edecd837293f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+-------------------+-------------------+\n",
            "|customer_id|usage_kwh|         start_time|     start_time_utc|\n",
            "+-----------+---------+-------------------+-------------------+\n",
            "|          1|      4.2|2025-09-01 19:45:00|2025-09-02 02:45:00|\n",
            "|          1|      2.3|2025-09-02 02:29:00|2025-09-02 09:29:00|\n",
            "|          1|      6.4|2025-09-02 20:12:00|2025-09-03 03:12:00|\n",
            "|          2|      3.7|2025-09-02 15:33:00|2025-09-02 22:33:00|\n",
            "|          3|      8.3|2025-09-02 21:22:00|2025-09-03 04:22:00|\n",
            "|          4|      3.1|2025-09-02 23:42:00|2025-09-03 06:42:00|\n",
            "|          4|      1.2|2025-09-03 04:30:00|2025-09-03 11:30:00|\n",
            "|          5|      5.0|2025-09-03 05:19:00|2025-09-03 12:19:00|\n",
            "+-----------+---------+-------------------+-------------------+\n",
            "\n",
            "+----------+-----------+--------------+\n",
            "|      date|customer_id|sum(usage_kwh)|\n",
            "+----------+-----------+--------------+\n",
            "|2025-09-02|          1|           6.5|\n",
            "|2025-09-02|          2|           3.7|\n",
            "|2025-09-03|          1|           6.4|\n",
            "|2025-09-03|          3|           8.3|\n",
            "|2025-09-03|          4|           4.3|\n",
            "|2025-09-03|          5|           5.0|\n",
            "+----------+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ADF(test_df_agg, expected_df)\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "hznV336UBjNs",
        "outputId": "2217c0e4-2586-4b9e-bbb1-375bd87bd228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pyspark' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3013658578.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#ADF(test_df_agg, expected_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pyspark' is not defined"
          ]
        }
      ]
    }
  ]
}